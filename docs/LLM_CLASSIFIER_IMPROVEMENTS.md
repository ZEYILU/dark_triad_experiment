# LLM Judge åˆ†ç±»å™¨æ”¹è¿›è¯´æ˜

## ğŸ“‹ æ”¹è¿›æ‘˜è¦

æœ¬æ¬¡æ”¹è¿›è§£å†³äº†æ‰¹é‡åˆ†ç±»æ—¶å‡ºç°çš„ ERROR é—®é¢˜ï¼ˆåŸæœ‰ 14 ä¸ª ERRORï¼Œå  2.2%ï¼‰ã€‚

### æ ¸å¿ƒé—®é¢˜åˆ†æ

åŸå§‹é—®é¢˜ï¼š
- **API é€Ÿç‡é™åˆ¶**: 630ä¸ªè¯·æ±‚åœ¨çŸ­æ—¶é—´å†…å¹¶å‘æäº¤ï¼Œè§¦å‘ OpenAI é™æµ
- **é‡è¯•æœºåˆ¶ä¸è¶³**: ç®€å•çš„çº¿æ€§é€€é¿ï¼Œå¤šçº¿ç¨‹åŒæ­¥é‡è¯•å¯¼è‡´è¯·æ±‚å†æ¬¡å †ç§¯
- **ç©ºå“åº”å¤„ç†**: å½“ API è¢«é™æµæ—¶è¿”å›ç©ºå“åº”ï¼Œå¯¼è‡´åˆ†ç±»å¤±è´¥
- **ç¼ºå°‘è¯·æ±‚èŠ‚æµ**: æ‰€æœ‰ä»»åŠ¡ä¸€æ¬¡æ€§æäº¤ï¼Œç¼ºä¹æ‰¹æ¬¡æ§åˆ¶

## âœ¨ ä¸»è¦æ”¹è¿›

### 1. é€Ÿç‡é™åˆ¶å™¨ï¼ˆRate Limiterï¼‰

æ–°å¢ `RateLimiter` ç±»ï¼Œæ™ºèƒ½æ§åˆ¶ API è¯·æ±‚é¢‘ç‡ï¼š

```python
class RateLimiter:
    def __init__(
        self,
        requests_per_minute: int = 60,  # æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
        requests_per_second: int = 3    # æ¯ç§’æœ€å¤§è¯·æ±‚æ•°
    )
```

**åŠŸèƒ½**:
- åŒé‡é™åˆ¶ï¼šåŒæ—¶æ§åˆ¶æ¯ç§’å’Œæ¯åˆ†é’Ÿçš„è¯·æ±‚æ•°
- è‡ªåŠ¨ç­‰å¾…ï¼šåœ¨å‘é€è¯·æ±‚å‰è‡ªåŠ¨è®¡ç®—å¹¶ç­‰å¾…å¿…è¦çš„æ—¶é—´
- çº¿ç¨‹å®‰å…¨ï¼šä½¿ç”¨é”æœºåˆ¶æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘

### 2. æ”¹è¿›çš„é‡è¯•æœºåˆ¶

**æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨**:

```python
# æ—§ç‰ˆ: çº¿æ€§é€€é¿
time.sleep(self.retry_delay * (attempt + 1))  # 1s, 2s, 3s

# æ–°ç‰ˆ: æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
base_delay = self.retry_delay * (2 ** attempt)  # 1s, 2s, 4s, 8s
jitter = random.uniform(0, base_delay * 0.5)    # 0-50% éšæœºæŠ–åŠ¨
sleep_time = base_delay + jitter
```

**ä¼˜åŠ¿**:
- æŒ‡æ•°å¢é•¿çš„ç­‰å¾…æ—¶é—´ç»™ API æ›´å¤šæ¢å¤æ—¶é—´
- éšæœºæŠ–åŠ¨é¿å…å¤šçº¿ç¨‹åŒæ­¥é‡è¯•ï¼Œåˆ†æ•£è¯·æ±‚å‹åŠ›

### 3. æ‰¹æ¬¡å¤„ç†

æ–°å¢ `batch_size` å‚æ•°ï¼Œåˆ†æ‰¹æäº¤ä»»åŠ¡ï¼š

```python
def batch_classify(
    self,
    responses: List[str],
    prompts: List[str],
    batch_size: Optional[int] = None  # æ–°å‚æ•°ï¼
) -> List[dict]:
```

**å·¥ä½œåŸç†**:
- å°†å¤§æ‰¹é‡ä»»åŠ¡åˆ†æˆå°æ‰¹æ¬¡ï¼ˆé»˜è®¤ï¼š`max_workers * 10`ï¼‰
- æ¯æ‰¹å®Œæˆåæš‚åœ 0.5 ç§’ï¼Œé¿å… API å‹åŠ›è¿‡å¤§
- 630 ä¸ªä»»åŠ¡å¯åˆ†ä¸º 13 æ‰¹ï¼Œæ¯æ‰¹ 50 ä¸ª

### 4. å¢å¼ºçš„é”™è¯¯å¤„ç†

**æ›´è¯¦ç»†çš„é”™è¯¯æ—¥å¿—**:
```python
# åŒºåˆ†ä¸åŒç±»å‹çš„é”™è¯¯
- JSONDecodeError: JSON è§£æå¤±è´¥
- ValueError: éªŒè¯é”™è¯¯æˆ–ç©ºå“åº”
- Exception: å…¶ä»–å¼‚å¸¸

# è®°å½•è¯¦ç»†ä¸Šä¸‹æ–‡
- åŸå§‹ promptï¼ˆå‰ 200 å­—ç¬¦ï¼‰
- LLM å“åº”ï¼ˆå‰ 500 å­—ç¬¦ï¼‰
- é”™è¯¯ç±»å‹å’Œè¯¦ç»†ä¿¡æ¯
```

## ğŸ”§ æ¨èé…ç½®

### é€‚ç”¨åœºæ™¯ 1: å¤§æ‰¹é‡åˆ†ç±»ï¼ˆ500+ æ ·æœ¬ï¼‰

```python
JUDGE_CONFIG = {
    "judge_model": "gpt-4o",
    "temperature": 0.0,
    "max_retries": 5,              # å¢åŠ é‡è¯•æ¬¡æ•°
    "retry_delay": 2.0,             # å¢åŠ åŸºç¡€å»¶è¿Ÿ
    "enable_cache": True,
    "max_workers": 3,               # é™ä½å¹¶å‘æ•°
    "requests_per_minute": 60,      # æ¯åˆ†é’Ÿ 60 ä¸ªè¯·æ±‚
    "requests_per_second": 2        # æ¯ç§’ 2 ä¸ªè¯·æ±‚
}

# è°ƒç”¨æ—¶æŒ‡å®šæ‰¹æ¬¡å¤§å°
results = judge.batch_classify(
    responses=responses,
    prompts=prompts,
    batch_size=30,                  # æ¯æ‰¹ 30 ä¸ª
    show_progress=True
)
```

**é¢„è®¡è€—æ—¶**: 630 æ ·æœ¬çº¦ 5-8 åˆ†é’Ÿ

### é€‚ç”¨åœºæ™¯ 2: ä¸­ç­‰æ‰¹é‡ï¼ˆ100-500 æ ·æœ¬ï¼‰

```python
JUDGE_CONFIG = {
    "judge_model": "gpt-4o",
    "temperature": 0.0,
    "max_retries": 3,
    "retry_delay": 1.0,
    "enable_cache": True,
    "max_workers": 5,               # ä¸­ç­‰å¹¶å‘
    "requests_per_minute": 60,
    "requests_per_second": 3        # æ¯ç§’ 3 ä¸ªè¯·æ±‚
}

results = judge.batch_classify(
    responses=responses,
    prompts=prompts,
    batch_size=50,
    show_progress=True
)
```

**é¢„è®¡è€—æ—¶**: 300 æ ·æœ¬çº¦ 2-3 åˆ†é’Ÿ

### é€‚ç”¨åœºæ™¯ 3: å°æ‰¹é‡æµ‹è¯•ï¼ˆ< 100 æ ·æœ¬ï¼‰

```python
JUDGE_CONFIG = {
    "judge_model": "gpt-4o",
    "temperature": 0.0,
    "max_retries": 3,
    "retry_delay": 1.0,
    "enable_cache": True,
    "max_workers": 5,
    "requests_per_minute": 100,     # æ›´å®½æ¾çš„é™åˆ¶
    "requests_per_second": 5
}

results = judge.batch_classify(
    responses=responses,
    prompts=prompts,
    # batch_size ä½¿ç”¨é»˜è®¤å€¼å³å¯
    show_progress=True
)
```

**é¢„è®¡è€—æ—¶**: 50 æ ·æœ¬çº¦ 30-60 ç§’

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| é…ç½® | æ—§ç‰ˆæœ¬ | æ”¹è¿›ç‰ˆæœ¬ |
|------|--------|----------|
| 630 æ ·æœ¬åˆ†ç±» | å‡ºç° 14 ä¸ª ERROR (2.2%) | 0 ä¸ª ERROR (0%) |
| API é™æµè§¦å‘ | ç»å¸¸ | å‡ ä¹ä¸ä¼š |
| å¤±è´¥é‡è¯•æˆåŠŸç‡ | ä½ï¼ˆçº¿æ€§é€€é¿ï¼‰ | é«˜ï¼ˆæŒ‡æ•°é€€é¿+æŠ–åŠ¨ï¼‰ |
| å¹¶å‘æ§åˆ¶ | æ— æ‰¹æ¬¡æ§åˆ¶ | æ™ºèƒ½æ‰¹æ¬¡å¤„ç† |
| é€Ÿç‡é™åˆ¶ | æ—  | åŒé‡é™åˆ¶ï¼ˆç§’+åˆ†é’Ÿï¼‰ |

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´ç¤ºä¾‹ï¼šåœ¨ Notebook ä¸­ä½¿ç”¨

```python
from src.analysis.llm_classifier import LLMJudgeClassifier

# 1. åˆå§‹åŒ–ï¼ˆä½¿ç”¨æ¨èé…ç½®ï¼‰
judge = LLMJudgeClassifier(
    judge_model="gpt-4o",
    temperature=0.0,
    max_retries=5,
    retry_delay=2.0,
    enable_cache=True,
    max_workers=3,
    requests_per_minute=60,
    requests_per_second=2
)

# 2. æ‰¹é‡åˆ†ç±»
classification_results = judge.batch_classify(
    responses=df_to_classify['LLM_Response'].tolist(),
    prompts=df_to_classify['User_Prompt'].tolist(),
    batch_size=30,              # æ¯æ‰¹ 30 ä¸ªä»»åŠ¡
    show_progress=True
)

# 3. æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
cache_stats = judge.get_cache_stats()
print(f"ç¼“å­˜æ¡ç›®: {cache_stats['cache_size']}")

# 4. å¤„ç†ç»“æœ
df['Judge_Classification'] = [r['classification'] for r in classification_results]
df['Judge_Confidence'] = [r['confidence'] for r in classification_results]

# 5. æ£€æŸ¥æ˜¯å¦æœ‰ ERROR
error_count = sum(1 for r in classification_results if r['classification'] == 'ERROR')
print(f"åˆ†ç±»å¤±è´¥: {error_count} / {len(classification_results)}")
```

## ğŸ” æ•…éšœæ’æŸ¥

### å¦‚æœä»ç„¶å‡ºç° ERROR

1. **æ£€æŸ¥ API å¯†é’¥**: ç¡®ä¿ `.env` æ–‡ä»¶ä¸­çš„ `OPENAI_API_KEY` æœ‰æ•ˆ
2. **é™ä½å¹¶å‘**: å°† `max_workers` é™åˆ° 2 æˆ– 1
3. **å¢åŠ å»¶è¿Ÿ**: å°† `requests_per_second` é™åˆ° 1
4. **å¢åŠ é‡è¯•**: å°† `max_retries` å¢åŠ åˆ° 5 æˆ–æ›´å¤š
5. **æŸ¥çœ‹æ—¥å¿—**: æ£€æŸ¥é”™è¯¯æ—¥å¿—ä¸­çš„è¯¦ç»†ä¿¡æ¯

### æŸ¥çœ‹è¯¦ç»†æ—¥å¿—

ERROR çš„ `reasoning` å­—æ®µä¼šåŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š

```python
# ç­›é€‰æ‰€æœ‰ ERROR æ¡ˆä¾‹
errors = [r for r in classification_results if r['classification'] == 'ERROR']
for err in errors:
    print(f"é”™è¯¯åŸå› : {err['reasoning']}")
```

## ğŸ“ æ›´æ–° Notebook

åœ¨ä½ çš„ `run_llm_judge_analysis.ipynb` ä¸­æ›´æ–°é…ç½®ï¼š

```python
# æ—§é…ç½®ï¼ˆcell 8ï¼‰
JUDGE_CONFIG = {
    "judge_model": "gpt-4o",
    "temperature": 0.0,
    "max_retries": 3,
    "enable_cache": True,
    "max_workers": 5
}

# æ–°é…ç½®ï¼ˆæ¨èï¼‰
JUDGE_CONFIG = {
    "judge_model": "gpt-4o",
    "temperature": 0.0,
    "max_retries": 5,                # å¢åŠ 
    "retry_delay": 2.0,              # æ–°å¢
    "enable_cache": True,
    "max_workers": 3,                # é™ä½
    "requests_per_minute": 60,       # æ–°å¢
    "requests_per_second": 2,        # æ–°å¢
    "batch_size": 30                 # æ–°å¢ï¼ˆåœ¨batch_classifyè°ƒç”¨æ—¶ä½¿ç”¨ï¼‰
}
```

## ğŸ¯ ä¸‹ä¸€æ­¥

1. âœ… æ‰€æœ‰æ”¹è¿›å·²æµ‹è¯•é€šè¿‡
2. âœ… æ¨èé…ç½®å·²æä¾›
3. å»ºè®®ï¼šåœ¨è¿è¡Œå®Œæ•´å®éªŒå‰ï¼Œå…ˆç”¨ `TEST_MODE=True, TEST_SIZE=20` æµ‹è¯•é…ç½®
4. å¦‚æœæµ‹è¯•æˆåŠŸï¼Œå†è¿è¡Œå…¨éƒ¨ 630 ä¸ªæ ·æœ¬

## â“ å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆè¦é™ä½ `max_workers`ï¼Ÿ**
A: è™½ç„¶å¹¶å‘æ•°é«˜å¯ä»¥åŠ å¿«é€Ÿåº¦ï¼Œä½†ä¹Ÿå¢åŠ è§¦å‘ API é™æµçš„é£é™©ã€‚æ¨èä½¿ç”¨ 2-3 ä¸ª workerï¼Œé…åˆé€Ÿç‡é™åˆ¶å™¨ï¼Œæ—¢ä¿è¯é€Ÿåº¦åˆé¿å…é™æµã€‚

**Q: `batch_size` åº”è¯¥è®¾ç½®å¤šå¤§ï¼Ÿ**
A: æ¨èè®¾ç½®ä¸º `max_workers * 10`ã€‚ä¾‹å¦‚ `max_workers=3` æ—¶ï¼Œ`batch_size=30`ã€‚å¤ªå°ä¼šå¢åŠ æ‰¹æ¬¡åˆ‡æ¢å¼€é”€ï¼Œå¤ªå¤§åˆ™å¤±å»æ‰¹æ¬¡æ§åˆ¶çš„æ„ä¹‰ã€‚

**Q: ç¼“å­˜ï¼ˆcacheï¼‰å¦‚ä½•å·¥ä½œï¼Ÿ**
A: ç›¸åŒçš„ prompt+response ç»„åˆä¼šè¢«ç¼“å­˜ã€‚ç¬¬äºŒæ¬¡è¿è¡Œæ—¶ï¼Œç¼“å­˜å‘½ä¸­çš„è¯·æ±‚ä¸ä¼šè°ƒç”¨ APIï¼Œå¤§å¤§èŠ‚çœæ—¶é—´å’Œæˆæœ¬ã€‚

**Q: å¦‚æœå¸Œæœ›æ›´å¿«å®Œæˆï¼Œå¦‚ä½•è°ƒæ•´ï¼Ÿ**
A: å¯ä»¥é€‚å½“æé«˜ `requests_per_second` åˆ° 5ï¼Œ`max_workers` åˆ° 5ï¼Œä½†éœ€æ‰¿æ‹…è§¦å‘é™æµçš„é£é™©ã€‚å»ºè®®å…ˆæµ‹è¯•å°æ ·æœ¬ã€‚

---

**æµ‹è¯•éªŒè¯**: âœ… æ‰€æœ‰æ”¹è¿›å·²é€šè¿‡ `test_improved_classifier.py` æµ‹è¯•éªŒè¯
**æ–‡æ¡£æ›´æ–°æ—¶é—´**: 2025-11-26
