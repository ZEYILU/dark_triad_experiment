"""
é‡æ„åçš„å®éªŒè¿è¡Œè„šæœ¬
ä½¿ç”¨é¢å‘å¯¹è±¡çš„æ¨¡å—åŒ–æ¶æ„
"""

import sys
import os
from pathlib import Path
from datetime import datetime
import json
import pandas as pd

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.config import get_config
from src.llm import OpenAIClient, AnthropicClient
from src.data import DatasetLoader
from src.utils.logger import setup_logger


class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨ - ç®¡ç†æ•´ä¸ªå®éªŒæµç¨‹"""

    def __init__(self, test_mode: bool = False):
        """
        åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨

        Args:
            test_mode: æµ‹è¯•æ¨¡å¼ï¼ˆåªæµ‹è¯•å‰5ä¸ªpromptsï¼‰
        """
        self.config = get_config()
        self.logger = setup_logger("experiment", use_colors=True)
        self.test_mode = test_mode
        self.results = []

    def run(self, model_configs: list):
        """
        è¿è¡Œå®éªŒ

        Args:
            model_configs: æ¨¡å‹é…ç½®åˆ—è¡¨
                          æ¯ä¸ªå…ƒç´ æ˜¯å­—å…¸: {"type": "openai|anthropic", "name": "model-name"}
        """
        self.logger.info(f"å¼€å§‹å®éªŒ - {len(model_configs)}ä¸ªæ¨¡å‹")
        self.logger.info(f"æµ‹è¯•æ¨¡å¼: {'æ˜¯' if self.test_mode else 'å¦'}")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.config.ensure_directories()

        # åŠ è½½æ•°æ®é›†
        dataset_path = self.config.get_path("dataset")
        loader = DatasetLoader(dataset_path)

        try:
            limit = 5 if self.test_mode else None
            loader.load(limit=limit)
        except Exception as e:
            self.logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            return False

        # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
        for i, model_config in enumerate(model_configs, 1):
            self.logger.info(f"\n{'='*70}")
            self.logger.info(f"æµ‹è¯•æ¨¡å‹ {i}/{len(model_configs)}: {model_config['name']}")
            self.logger.info(f"{'='*70}")

            success = self._test_model(model_config, loader)

            if success:
                self.logger.info(f"æ¨¡å‹ {model_config['name']} æµ‹è¯•å®Œæˆ")
            else:
                self.logger.error(f"æ¨¡å‹ {model_config['name']} æµ‹è¯•å¤±è´¥")

        self.logger.info(f"\n{'='*70}")
        self.logger.info("å®éªŒå®Œæˆ!")
        self.logger.info(f"{'='*70}")

        return True

    def _test_model(self, model_config: dict, loader: DatasetLoader) -> bool:
        """
        æµ‹è¯•å•ä¸ªæ¨¡å‹

        Args:
            model_config: æ¨¡å‹é…ç½®
            loader: æ•°æ®åŠ è½½å™¨

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        model_type = model_config["type"]
        model_name = model_config["name"]

        # è·å–APIå¯†é’¥
        api_key = self.config.get_api_key(model_type)
        if not api_key:
            self.logger.error(f"æœªé…ç½®{model_type}çš„APIå¯†é’¥")
            return False

        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        try:
            if model_type == "openai":
                client = OpenAIClient(
                    model_name=model_name,
                    api_key=api_key,
                    temperature=0.7,
                    max_tokens=1000
                )
            elif model_type == "anthropic":
                client = AnthropicClient(
                    model_name=model_name,
                    api_key=api_key,
                    temperature=0.7,
                    max_tokens=1000
                )
            else:
                self.logger.error(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
                return False

        except Exception as e:
            self.logger.error(f"åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: {e}")
            return False

        # æµ‹è¯•æ‰€æœ‰prompts
        model_results = []
        total = len(loader)

        for idx in range(total):
            metadata = loader.get_metadata(idx)
            prompt = metadata['User Prompt']

            self.logger.info(f"[{idx+1}/{total}] æµ‹è¯• prompt: {metadata['ID']}")

            # è°ƒç”¨LLM
            response = client.call(prompt)

            # è®°å½•ç»“æœ
            result = {
                'ID': metadata['ID'],
                'Primary Trait': metadata['Primary Trait'],
                'Context': metadata['Context'],
                'Severity': metadata['Severity'],
                'User Prompt': prompt,
                'LLM Response': response if response else "ERROR: No response",
                'Model': model_name,
                'Timestamp': datetime.now().isoformat(),
                'Success': response is not None
            }

            model_results.append(result)

            # æ˜¾ç¤ºå“åº”é¢„è§ˆ
            if response:
                preview = response[:100] + "..." if len(response) > 100 else response
                self.logger.info(f"å“åº”: {preview}\n")

        # ä¿å­˜ç»“æœ
        return self._save_results(model_name, model_results)

    def _save_results(self, model_name: str, results: list) -> bool:
        """
        ä¿å­˜æµ‹è¯•ç»“æœ

        Args:
            model_name: æ¨¡å‹åç§°
            results: ç»“æœåˆ—è¡¨

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not results:
            self.logger.error("æ²¡æœ‰ç»“æœå¯ä¿å­˜")
            return False

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = self.config.get_path("results")

        # ä¿å­˜CSV
        csv_filename = f"{results_dir}/results_{model_name.replace('/', '_')}_{timestamp}.csv"
        df = pd.DataFrame(results)
        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        self.logger.info(f"CSVå·²ä¿å­˜: {csv_filename}")

        # ä¿å­˜JSON
        json_filename = f"{results_dir}/results_{model_name.replace('/', '_')}_{timestamp}.json"
        output_data = {
            "metadata": {
                "model": model_name,
                "total_prompts": len(results),
                "successful": sum(1 for r in results if r['Success']),
                "failed": sum(1 for r in results if not r['Success']),
                "timestamp": timestamp,
            },
            "results": results
        }

        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        self.logger.info(f"JSONå·²ä¿å­˜: {json_filename}")

        # æ‰“å°ç»Ÿè®¡
        self._print_statistics(results)

        return True

    def _print_statistics(self, results: list):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        df = pd.DataFrame(results)

        self.logger.info(f"\n{'='*60}")
        self.logger.info("ç»Ÿè®¡ä¿¡æ¯")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"æˆåŠŸ: {df['Success'].sum()}")
        self.logger.info(f"å¤±è´¥: {(~df['Success']).sum()}")
        self.logger.info(f"\næŒ‰ç‰¹è´¨:")
        for trait, count in df['Primary Trait'].value_counts().items():
            self.logger.info(f"  {trait}: {count}")
        self.logger.info(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ğŸ§ª Dark Triad Experiment - é‡æ„ç‰ˆæœ¬ v2.0")
    print("="*70)

    # è·å–é…ç½®
    config = get_config()

    # éªŒè¯APIå¯†é’¥
    print("\næ£€æŸ¥APIå¯†é’¥...")
    api_status = config.validate_api_keys()
    for provider, valid in api_status.items():
        status = "âœ…" if valid else "âŒ"
        print(f"  {status} {provider.upper()}")

    if not any(api_status.values()):
        print("\nâŒ æœªé…ç½®ä»»ä½•APIå¯†é’¥")
        print("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®APIå¯†é’¥")
        sys.exit(1)

    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    print("\n" + "="*70)
    print("é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. å¿«é€Ÿæµ‹è¯• (GPT-3.5-Turbo)")
    print("2. æ ‡å‡†æµ‹è¯• (GPT-4 + Claude Haiku)")
    print("3. å®Œæ•´æµ‹è¯• (GPT-4 + GPT-3.5 + Claude Haiku)")
    print("4. æµ‹è¯•æ¨¡å¼ (åªæµ‹è¯•å‰5ä¸ªprompts)")
    print("="*70)

    choice = input("\né€‰æ‹© (1-4): ").strip()

    model_configs = []
    test_mode = False

    if choice == "1":
        model_configs = [
            {"type": "openai", "name": "gpt-3.5-turbo"}
        ]
    elif choice == "2":
        model_configs = [
            {"type": "openai", "name": "gpt-4"},
            {"type": "anthropic", "name": "claude-3-haiku-20240307"}
        ]
    elif choice == "3":
        model_configs = [
            {"type": "openai", "name": "gpt-4"},
            {"type": "openai", "name": "gpt-3.5-turbo"},
            {"type": "anthropic", "name": "claude-3-haiku-20240307"}
        ]
    elif choice == "4":
        print("\nâš ï¸ æµ‹è¯•æ¨¡å¼ï¼šåªæµ‹è¯•å‰5ä¸ªprompts")
        test_mode = True
        model_configs = [
            {"type": "openai", "name": "gpt-3.5-turbo"}
        ]
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        sys.exit(1)

    # æ˜¾ç¤ºé…ç½®
    print(f"\nå°†æµ‹è¯• {len(model_configs)} ä¸ªæ¨¡å‹:")
    for i, cfg in enumerate(model_configs, 1):
        print(f"  {i}. {cfg['name']} ({cfg['type']})")

    # ç¡®è®¤
    confirm = input("\nç¡®è®¤å¼€å§‹? (y/n): ").strip().lower()
    if confirm != 'y':
        print("å·²å–æ¶ˆ")
        sys.exit(0)

    # è¿è¡Œå®éªŒ
    runner = ExperimentRunner(test_mode=test_mode)
    success = runner.run(model_configs)

    if success:
        print("\nä¸‹ä¸€æ­¥:")
        print("1. åˆ†æç»“æœ: python scripts/analyze.py")
        print("2. å¯è§†åŒ–: python visualize_results.py")
    else:
        print("\nå®éªŒå‡ºé”™ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        sys.exit(1)


if __name__ == "__main__":
    main()
